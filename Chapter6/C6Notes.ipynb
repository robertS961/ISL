{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Chapter 6 Linear Model Selection and Regularization"
      ],
      "metadata": {
        "id": "oVHXLbKOOolf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 6.0 - Introduction\n",
        "\n",
        "* We have been using least squares and we are going to look at alternate ways to do perform linear regression\n",
        "\n",
        "* **Least Squares** - Model Prediction : is great when we have n >p for n observations and p predictors. This leaves us with a low variance. However when n is close to p then we tend to have a higher degree of variance from overfitting. When  p> n then out model most certainly overfits and is in accurate.\n",
        "\n",
        "* **Least Squares** - Model Interpretability: We want certain predictors who are irrelevant to have coefficent to 0, however with least squares this rarely happens.\n",
        "\n",
        "* **Alternatives** -\n",
        "\n",
        ">> 1) Subset Selection - pick a subset of p who we think is related then perform least squares on the subset.\n",
        "\n",
        ">> 2) Shrinkage - Use all p predictors this time, however the p coefficients are shrunken towards 0 relative to the least square estimate. This can make some coefficients 0.\n",
        "\n",
        ">> 3)Dimension Reduction - Projecting the p predictors into M dimensional subspace where M < p(reducing p). Then use these M linear projections of p with linear regression.\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "tJa-7gjPOxX3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 6.1 Subset Selection\n",
        "* Go over how to create the most optimal subset\n",
        "* The two main methods are **best subset** and **stepwise model selection procedure**"
      ],
      "metadata": {
        "id": "QJcUxKHbO4c1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 6.1.1 Best Subset Selection\n",
        "\n",
        "* Algorithms for Best Subset -\n",
        "\n",
        " * Iterate from $1..p$ call it $i$. We calculate all models with $i$ predictors.\n",
        " *Out of those models we pick the best one($R^{2}$ ) call it $M_{i}$\n",
        " * Do this p times so we have $p M_{i}$ models\n",
        " * Test each using cross validation or on the validation test. The one who performs the lowest error is the best.\n",
        "\n",
        "* Cons: The Algo runs in $p2^{p}$ run time so it is limited computationally. When P becomes larger than 30 it is usually not recommended"
      ],
      "metadata": {
        "id": "2UBYDdECwHWG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 6.1.2 Stepwise Model Selection\n",
        "\n",
        "* Best subset Selection can also suffer from overfitting with a high p as this will increase the variance. It also suffers computationally from a high p.\n",
        "\n",
        "* Stepwise is a solid alternative to best subset selection\n"
      ],
      "metadata": {
        "id": "nt3z4_exwTTP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Forward Stepwise Selection\n",
        "\n",
        "* Algorithm for Forward Stepwise Selection -\n",
        "  * Start with the null model call it $M_{0}$\n",
        "  * Iterate from $0...p -1$ call it $i$.\n",
        "    * We add 1 predictor everytime and test all p - i predictors. The one with the lowest $R^{2}$ value we add. Call the select model $M_{i}$\n",
        "  * We have an array of p models and test them all on the validation set or use cross validation. The best result is our model\n",
        "* This will run in $p^{2}$ time instead of $2^{p}$ time\n",
        "* The con of forward stepwise is that if it selects A for var1 as the best then for p= 2 it must still use A. However for p=2 the most model might be B and C.  \n",
        "* Can be used when p >n\n"
      ],
      "metadata": {
        "id": "2VQYl7600KHd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Backward Stepwise Selection\n",
        "\n",
        "* Must like forward stepwise selection except instead of starting with the null model $M_{0}$ we still with all predictors in the model. Then we move one predictor per iteration that removes the least $R^{2}$ value\n",
        "\n",
        "* It has the same runtime and the same con as it is not guarenteed to select the best model.\n",
        "\n",
        "* Only viable if p < n. So not viable when p is extremely large\n"
      ],
      "metadata": {
        "id": "6SkXNAqi3jQg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Hybrid Approach\n",
        "\n",
        "* Start with forward stepwise selection and can use backward selection at anytime to remove uneeded variables"
      ],
      "metadata": {
        "id": "pcYHbJIx5zUd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 6.1.3 Choosing the Optimal Model\n",
        "\n",
        "* In the above algorithms we select the best model in the for loop by using $R^{2}$ and this works well on the training data. However we can't do this on the test data as it won't give us the best answer. Instead we can use:\n",
        "\n",
        "   * We can make an adjustment to the training error to account for overfitting\n",
        "   * We can directly test the test error using a validation or cross validation approach"
      ],
      "metadata": {
        "id": "MvNm_n_56z8j"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### $C_{p}$, AIC, BIC and Adjusted $R^{2}$\n",
        "\n",
        "1. $C_{p}$\n",
        "1. BIC - Bayes Information Criteria\n",
        "1. AIC - Akiake Information Criteria\n",
        "1. Adjusted $R_{2}$\n",
        "\n",
        "1) Basically gives a pentalty for more predictors choosen so we want to pick the model with the lowest $C_{P}$ value\n",
        "\n",
        "2) Basically the same as 1) and they are porportional to each other\n",
        "\n",
        "3) Use log(n) term instead of a 2 term used by 1) and 2) so it generally selects smaller predictor models then 1) and 2).\n",
        "\n",
        "4) We want a large value to indicated a low test error rate unlike 1,2,3 where we want a small value to indicate a low test error rate. It also punishes us for the more predictors we fit\n",
        "\n",
        "* All the above are solid choices but adjusted $R^{2}$ is seen as the least motivated in stats thoery. You can also used 2,3 with other formulas than least squares."
      ],
      "metadata": {
        "id": "UASgkVkK7pJx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Validation and Cross Validation\n",
        "\n",
        "* In the past the Cross Validation approach was less favored compared to 1,2,3 since they are less computational intense. However now a days we can perform the ross Validation.\n",
        "\n",
        "* **One standard error rule** - Calculate the standard error of the estimate test from MSE for each model size.  Then select the smallest model(loest p choosen) for which the estimate test error is within one standard error of the lowest point on the curve.\n",
        "\n",
        "* We do this because selecting a different validation test due to randomness or selecting different folds will surely give us a different result"
      ],
      "metadata": {
        "id": "c1AvCTfg7-6M"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 6.2 Shrinking Methods\n",
        "\n",
        "* Instead of picking a subset of p predictors, we will choose them all\n",
        "* We will also shrink coefficents towards 0. This can significantly reduce a models variance\n",
        "* Main two methods are:\n",
        ">1. Ridge Regression\n",
        ">1. Lasso"
      ],
      "metadata": {
        "id": "zFKxQSvaC3BQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 6.2.1 Ridge Regression"
      ],
      "metadata": {
        "id": "72EMx9_cDeMN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* Has a shrinkage penalty ontop of the normal RSS. It is small when $B_{0} ... B_{p}$ are close to 0. Thus has the effect of shrinking numbers closer to 0 while others grow a higher rate.\n",
        "\n",
        "* a $ùõå$ is the coefficent in front of the shrinkage term to control its power. As $Œª$ grows so does the shrinkage. So you must make muliple test with muliple $Œª$ values"
      ],
      "metadata": {
        "id": "EWNfZHIhDmy1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Aplication to Credit Data\n",
        "\n",
        "* notice as $Œª$ grows so does the shrinkage, however sometimes it can do the opposite effect.\n",
        "\n",
        "* Must make sure to standardize our variables before applying this formual\n"
      ],
      "metadata": {
        "id": "pKRP04QkKP7n"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Why does Ridge Regression Improve Least Squares\n",
        "\n",
        "* IT decreases variance up to a certain point with a very small increase in bias. After a certain therhold tho the variance decrease is not worth the bias increase as the bias grows\n",
        "\n",
        "* We want to pick this choice in situations where there is high variance. There is higher variance when n and p are closely related or when the response and predictors are close to linear. Both cause high variance and we can limit this with ridge regression. This works since we start limiting the coefficents of same of the variables in the regression formula\n",
        "\n",
        "* Ridge regression is also O(n) time so it is faster than the subset method for large P"
      ],
      "metadata": {
        "id": "USl31h8DKf-Z"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 6.2.2 The Lasso"
      ],
      "metadata": {
        "id": "NuTAfJX6M3tc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* Ridge regression has one disadvantage. It must included all the predictors and it will bring them close to 0, but not 0. What if we want a subset of the variables only including the most important ones.\n",
        "\n",
        "* The lasso fixes this problem with variable selection. Instead of having a regression penalty that shrinks value it has a variable selection that nullifies values.\n",
        "\n",
        "* Still need to pick the best $Œª$ value which you can use cross validation for.\n",
        "\n",
        "* Lasso is also computationally similar to Rdige Regression so it is a solid alternative to best subset selection."
      ],
      "metadata": {
        "id": "23tFNPmgOIXL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Ridge Regression is Lasso\n",
        "\n",
        "* Ridge regression has an advantage when all the predictors are used. While Lasso has an advantage if the best fit has not all the predictors being used(easier readability too). Both have their advantages\n",
        "\n",
        "* Best are a solid computational alternative to best subset as they run in O(n). Great for huge p sized models\n",
        "\n",
        "* Both do a great job at reducing variance for a small increase in bias for models where p is roughly around n."
      ],
      "metadata": {
        "id": "-e-Sn5KGPJm4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 6.2.3 Selecting the Tuning Parameters\n",
        "\n",
        "* We can use cross validation to determine the best value for $Œª$ then go back in and retest with the best $Œª$\n",
        "\n",
        "* So we pick $ Œª$ based on cross validation on the lasso algorithm. Then go back into that alogrithm and find the best formula using the correct $Œª$."
      ],
      "metadata": {
        "id": "cVJ0itTfQCEx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 6.3 Dimensional Reduction Method\n",
        "\n",
        "* The previous methods have used either a subset of the predictors or all the predictors then shrunk them to 0. This method will transform the predictors first then fit them.\n",
        "\n",
        "* It is called dimesnional reducion since we are reducing the p predictors to $M$ predictors where $M$ < p.\n",
        "\n",
        "* We can determine these new coefficents $Z_{1}.....Z_{M}$ by using:\n",
        "> 1. Principal Components\n",
        "> 1. Partial Least Squats"
      ],
      "metadata": {
        "id": "2na2112FBwXM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 6.3.1 Principal Components Regression\n",
        "\n",
        "* First Principal Compoment - direction along the data where the data varries the most\n",
        "\n",
        "* Second Principal Component - must be orthogonal to the first principal component. Need zero correlation between $Z_{1}$ and  $Z_{2}$\n",
        "\n",
        "* Principal Component Loadings - are the coefficents we discover for the equation.\n",
        "\n",
        "* However the principal components can also be the furthest away instead of closest.\n",
        "\n",
        "* Can construct up to $p$ principal components, but we might not want that many. They can be less useful the more we create."
      ],
      "metadata": {
        "id": "21Nsm782F-46"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### The Principal Component Regression Approach\n",
        "\n",
        "* Construct the first $M$ principal comments $Z_{1}..... Z_{m}$. Then use these as predictors in the linear regression model that is fit using least squares.\n",
        "\n",
        "* Normally only need a few predictors to make an accurate prediction.\n",
        "\n",
        "* The assumption is also based on the variables that see the most movement are the ones we think will have the most direction with Y.\n",
        "\n",
        "* This prevents overfitting by sliming down the parameters and picking ones that we believe cause large changes in Y. If we pick all the important ones than it should be better than least squares.\n",
        "\n",
        "* We should think of Ridge Regression as a progression of PCR. It is not a variable selector since each variable is a linear combination of the previous variables.\n",
        "\n",
        "* We usually use Cross Validation to choose M predictors.\n",
        "\n",
        "* Should standardize the variables before you perform PCR"
      ],
      "metadata": {
        "id": "dgMFf6uDQjMY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 6.3.2 Partial Least Squares\n",
        "\n",
        "* PCR is an unsupervised method since it doesn't matter about the response Y. This is a draw back since technically the best predictor for the predictors, wont be the best predictor for the response.\n",
        "\n",
        "* Supervised approach to PCR. It does the same thing as PCR using $M$ predictors using linear combinations of each other. However it uses the response Y for these $M$ features.\n",
        "\n",
        "* We once again determine M by cross validation then, and standardize the predictors beforehand. Then we run it M times to generate M predictors then use those M predictor coefficents to run least square.\n",
        "\n",
        "* Normally equal to PCR since PLS reduces bias but increase variance. So its normally a wash"
      ],
      "metadata": {
        "id": "8WqmE2ZDYceR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 6.4 Considerations in High Demensional Data"
      ],
      "metadata": {
        "id": "n4GQt0eUb-Lv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 6.4.1 High Dimensional Data\n",
        "\n",
        "* Most problems in history for statistics have used low dimensionality which means large n for a small p.\n",
        "\n",
        "* Now that computers can collect much data we can gather much larger number of predictors. Now n is limited by storage space for money.\n",
        "\n",
        "* High dimensional is referred to as when p is larger than n."
      ],
      "metadata": {
        "id": "CoDIaZ3YccJ_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 6.4.2 What Goes Wrong in High Dimensional?\n",
        "\n",
        "* First the data will be overfitted so extremely high variance. This is because there isnt enough data for each predictor so we perfectly fit the data.\n",
        "\n",
        "* It will be over flexible and fit the training data extremely well, but will fit the test data extremely poorly.\n",
        "\n",
        "* Also must be careful of unrelated variables to the response as they will increase the $R^{2}$ on the train and lower the MSE but will perform poorly on the test data.\n",
        "\n",
        "* Also AIC, BIC, and $C_{p}$ are not good for High Dimensional since $œÉ^{2}$ estimation will equal 0 at high dimensions\n"
      ],
      "metadata": {
        "id": "uk8phrAIcgLw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 6.4.3 Regression in high dimensions\n",
        "\n",
        "* Turns out the ridge selection, forward stepwise selection, lasso, and principle component regression work well in high dimensions\n",
        "\n",
        "* Curse of high dimensionality means you think you want more features. However normally more features leads to more test error. Unless the features are actually related to the response.\n",
        "\n",
        "* But adding noise predictors will make the model worse.\n",
        "\n",
        "* Fitting more relevant features may bring more trouble if the variance brought on doesn't out weight the bias\n",
        "\n"
      ],
      "metadata": {
        "id": "sEX8YwoHiWsY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "6.4.4 Interpreting Results in High Dimensions\n",
        "\n",
        "* We need to watch out for collinearity as it will be more intense with higher predictors. Also its tought to know which are correlated.\n",
        "\n",
        "* This model can still be useful for predicting the response, however these individual predictors might not be the only useful features.\n",
        "\n",
        "* Make sure to never use the training set to back up results for why its accurate. Use cross validation and train/test split\n"
      ],
      "metadata": {
        "id": "yJZZK8VRieH2"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ERRrBBkHl7dT"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}