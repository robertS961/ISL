{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Chapter 6 Linear Model Selection and Regularization"
      ],
      "metadata": {
        "id": "oVHXLbKOOolf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 6.0 - Introduction\n",
        "\n",
        "* We have been using least squares and we are going to look at alternate ways to do perform linear regression\n",
        "\n",
        "* **Least Squares** - Model Prediction : is great when we have n >p for n observations and p predictors. This leaves us with a low variance. However when n is close to p then we tend to have a higher degree of variance from overfitting. When  p> n then out model most certainly overfits and is in accurate.\n",
        "\n",
        "* **Least Squares** - Model Interpretability: We want certain predictors who are irrelevant to have coefficent to 0, however with least squares this rarely happens.\n",
        "\n",
        "* **Alternatives** -\n",
        "\n",
        ">> 1) Subset Selection - pick a subset of p who we think is related then perform least squares on the subset.\n",
        "\n",
        ">> 2) Shrinkage - Use all p predictors this time, however the p coefficients are shrunken towards 0 relative to the least square estimate. This can make some coefficients 0.\n",
        "\n",
        ">> 3)Dimension Reduction - Projecting the p predictors into M dimensional subspace where M < p(reducing p). Then use these M linear projections of p with linear regression.\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "tJa-7gjPOxX3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 6.1 Subset Selection\n",
        "* Go over how to create the most optimal subset\n",
        "* The two main methods are **best subset** and **stepwise model selection procedure**"
      ],
      "metadata": {
        "id": "QJcUxKHbO4c1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 6.1.1 Best Subset Selection\n",
        "\n",
        "* Algorithms for Best Subset -\n",
        "\n",
        " * Iterate from $1..p$ call it $i$. We calculate all models with $i$ predictors.\n",
        " *Out of those models we pick the best one($R^{2}$ ) call it $M_{i}$\n",
        " * Do this p times so we have $p M_{i}$ models\n",
        " * Test each using cross validation or on the validation test. The one who performs the lowest error is the best.\n",
        "\n",
        "* Cons: The Algo runs in $p2^{p}$ run time so it is limited computationally. When P becomes larger than 30 it is usually not recommended"
      ],
      "metadata": {
        "id": "2UBYDdECwHWG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 6.1.2 Stepwise Model Selection\n",
        "\n",
        "* Best subset Selection can also suffer from overfitting with a high p as this will increase the variance. It also suffers computationally from a high p.\n",
        "\n",
        "* Stepwise is a solid alternative to best subset selection\n"
      ],
      "metadata": {
        "id": "nt3z4_exwTTP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Forward Stepwise Selection\n",
        "\n",
        "* Algorithm for Forward Stepwise Selection -\n",
        "  * Start with the null model call it $M_{0}$\n",
        "  * Iterate from $0...p -1$ call it $i$.\n",
        "    * We add 1 predictor everytime and test all p - i predictors. The one with the lowest $R^{2}$ value we add. Call the select model $M_{i}$\n",
        "  * We have an array of p models and test them all on the validation set or use cross validation. The best result is our model\n",
        "* This will run in $p^{2}$ time instead of $2^{p}$ time\n",
        "* The con of forward stepwise is that if it selects A for var1 as the best then for p= 2 it must still use A. However for p=2 the most model might be B and C.  \n",
        "* Can be used when p >n\n"
      ],
      "metadata": {
        "id": "2VQYl7600KHd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Backward Stepwise Selection\n",
        "\n",
        "* Must like forward stepwise selection except instead of starting with the null model $M_{0}$ we still with all predictors in the model. Then we move one predictor per iteration that removes the least $R^{2}$ value\n",
        "\n",
        "* It has the same runtime and the same con as it is not guarenteed to select the best model.\n",
        "\n",
        "* Only viable if p < n. So not viable when p is extremely large\n"
      ],
      "metadata": {
        "id": "6SkXNAqi3jQg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Hybrid Approach\n",
        "\n",
        "* Start with forward stepwise selection and can use backward selection at anytime to remove uneeded variables"
      ],
      "metadata": {
        "id": "pcYHbJIx5zUd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 6.1.3 Choosing the Optimal Model\n",
        "\n",
        "* In the above algorithms we select the best model in the for loop by using $R^{2}$ and this works well on the training data. However we can't do this on the test data as it won't give us the best answer. Instead we can use:\n",
        "\n",
        "   * We can make an adjustment to the training error to account for overfitting\n",
        "   * We can directly test the test error using a validation or cross validation approach"
      ],
      "metadata": {
        "id": "MvNm_n_56z8j"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### $C_{p}$, AIC, BIC and Adjusted $R^{2}$\n",
        "\n",
        "1. $C_{p}$\n",
        "1. BIC - Bayes Information Criteria\n",
        "1. AIC - Akiake Information Criteria\n",
        "1. Adjusted $R_{2}$\n",
        "\n",
        "1) Basically gives a pentalty for more predictors choosen so we want to pick the model with the lowest $C_{P}$ value\n",
        "\n",
        "2) Basically the same as 1) and they are porportional to each other\n",
        "\n",
        "3) Use log(n) term instead of a 2 term used by 1) and 2) so it generally selects smaller predictor models then 1) and 2).\n",
        "\n",
        "4) We want a large value to indicated a low test error rate unlike 1,2,3 where we want a small value to indicate a low test error rate. It also punishes us for the more predictors we fit\n",
        "\n",
        "* All the above are solid choices but adjusted $R^{2}$ is seen as the least motivated in stats thoery. You can also used 2,3 with other formulas than least squares."
      ],
      "metadata": {
        "id": "UASgkVkK7pJx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Validation and Cross Validation\n",
        "\n",
        "* In the past the Cross Validation approach was less favored compared to 1,2,3 since they are less computational intense. However now a days we can perform the ross Validation.\n",
        "\n",
        "* **One standard error rule** - Calculate the standard error of the estimate test from MSE for each model size.  Then select the smallest model(loest p choosen) for which the estimate test error is within one standard error of the lowest point on the curve.\n",
        "\n",
        "* We do this because selecting a different validation test due to randomness or selecting different folds will surely give us a different result"
      ],
      "metadata": {
        "id": "c1AvCTfg7-6M"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 6.2 Shrinking Methods\n",
        "\n",
        "* Instead of picking a subset of p predictors, we will choose them all\n",
        "* We will also shrink coefficents towards 0. This can significantly reduce a models variance\n",
        "* Main two methods are:\n",
        ">1. Ridge Regression\n",
        ">1. Lasso"
      ],
      "metadata": {
        "id": "zFKxQSvaC3BQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 6.2.1 Ridge Regression"
      ],
      "metadata": {
        "id": "72EMx9_cDeMN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* Has a shrinkage penalty ontop of the normal RSS. It is small when $B_{0} ... B_{p}$ are close to 0. Thus has the effect of shrinking numbers closer to 0 while others grow a higher rate.\n",
        "\n",
        "* a $ùõå$ is the coefficent in front of the shrinkage term to control its power. As $Œª$ grows so does the shrinkage. So you must make muliple test with muliple $Œª$ values"
      ],
      "metadata": {
        "id": "EWNfZHIhDmy1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Aplication to Credit Data\n",
        "\n",
        "* notice as $Œª$ grows so does the shrinkage, however sometimes it can do the opposite effect.\n",
        "\n",
        "* Must make sure to standardize our variables before applying this formual\n"
      ],
      "metadata": {
        "id": "pKRP04QkKP7n"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Why does Ridge Regression Improve Least Squares\n",
        "\n",
        "* IT decreases variance up to a certain point with a very small increase in bias. After a certain therhold tho the variance decrease is not worth the bias increase as the bias grows\n",
        "\n",
        "* We want to pick this choice in situations where there is high variance. There is higher variance when n and p are closely related or when the response and predictors are close to linear. Both cause high variance and we can limit this with ridge regression. This works since we start limiting the coefficents of same of the variables in the regression formula\n",
        "\n",
        "* Ridge regression is also O(n) time so it is faster than the subset method for large P"
      ],
      "metadata": {
        "id": "USl31h8DKf-Z"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 6.2.2 The Lasso"
      ],
      "metadata": {
        "id": "NuTAfJX6M3tc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* Ridge regression has one disadvantage. It must included all the predictors and it will bring them close to 0, but not 0. What if we want a subset of the variables only including the most important ones.\n",
        "\n",
        "* The lasso fixes this problem with variable selection. Instead of having a regression penalty that shrinks value it has a variable selection that nullifies values.\n",
        "\n",
        "* Still need to pick the best $Œª$ value which you can use cross validation for.\n",
        "\n",
        "* Lasso is also computationally similar to Rdige Regression so it is a solid alternative to best subset selection."
      ],
      "metadata": {
        "id": "23tFNPmgOIXL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Ridge Regression is Lasso\n",
        "\n",
        "* Ridge regression has an advantage when all the predictors are used. While Lasso has an advantage if the best fit has not all the predictors being used(easier readability too). Both have their advantages\n",
        "\n",
        "* Best are a solid computational alternative to best subset as they run in O(n). Great for huge p sized models\n",
        "\n",
        "* Both do a great job at reducing variance for a small increase in bias for models where p is roughly around n."
      ],
      "metadata": {
        "id": "-e-Sn5KGPJm4"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "cVJ0itTfQCEx"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}