{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Chapter 5 Resampling Methods"
      ],
      "metadata": {
        "id": "UtrICyNldn-L"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 5.0 Ideas\n",
        "* Resampling methods - Tool used to constantly resample the data to improve the accuracy of the model\n",
        "\n",
        "* Most commonly used ones are **Bootsraping** and **Cross Validation**\n",
        "\n",
        "* Highly costly computationally wise but in todays age, computers are extremely fast so less of an issue.\n",
        "\n",
        "* **Model Assesment**- assessing a models performance\n",
        "\n",
        "* **Model Selection** - selecting the correct level of flexibility\n",
        "\n",
        "* Use cross validation to assess a models test error rate to find its performance or dictate the level of flexibility that is best.\n",
        "\n",
        "* Use bootstrap most commonly used to provide a measure of accuracy of a parameter measurement or of a given statistical learning method\n",
        "\n"
      ],
      "metadata": {
        "id": "tcTX_K9Ie2-9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 5.1 Cross Validation\n",
        "\n",
        "* We use these techniques in the absences of a large data set. If we had a large data set then we could train test split and have it be accurate. However on a small data set our training data would likely not produce the most accurate results and our test error rate would be high."
      ],
      "metadata": {
        "id": "Z3FfiPKBfu05"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 5.1.1 The Validation Set Approach\n",
        "\n",
        "* This is the method we used in chapter 4 applied exercises. We randomly split the data into a training and validation(Test set). We pick the percent we want randomly selected for the validation set.\n",
        "\n",
        "* We can then train several different models with diff qadradic terms or colinear terms and see which perform best on the validation set\n",
        "\n",
        "* However since it is random there is some variability between choices. This is because certain points can sway the decision wether they get picked for validation or training. It can lead to high variable test error rates.\n",
        "\n",
        "* Since you are using half the data you can overfit or over estimate the training"
      ],
      "metadata": {
        "id": "FxvJ0RxHgYc0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 5.1.2 Leave one out Cross Validation\n",
        "\n",
        "* LOOCV splits the data in train, validate too except it leaves only one point out for validate. It does this for all n points then takes the average of all the MSE from 1.... n.\n",
        "\n",
        "* Tends to not overestimate the error rate since it uses n-1 points everytime instead of n/2 points.\n",
        "\n",
        "* There isn't any randomness in applying the procedure muliple times since it will always use the same points while Validation will yield different results for muliple tests since it has an element of randomness\n",
        "\n",
        "* The disadvantage is the model must be fit n times which is costly especially if n is large. There is a shortcut estimate we can use but it doesn't hold everytime."
      ],
      "metadata": {
        "id": "EG75LC5FZMfR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 5.1.3 K Fold Cross Validation\n",
        "\n",
        "* Dictate a certain number of folds k. Then we pick one of the k folds as the validation set and use MSE on the remaining k-1 folds. We do this again picking a diff k until k times. WE then average the k tests of MSE\n",
        "\n",
        "* Thus LOOCV is a type of K Fold Cross Validation where k == n\n",
        "\n",
        "* However if you make K smaller than there is a computational advantage\n",
        "\n",
        "* Also the smaller the k can aid in the bias - variance trade off.\n",
        "\n",
        "* Still possible that it overestimates or underestimates the test error, however discovering the minimum test error rate can help us select the best level of flexibility, disregarding the test error rate.\n"
      ],
      "metadata": {
        "id": "BMo4bVKza_CS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 5.1.4 Bias Variance Trade off\n",
        "\n",
        "* LOOCV has extremely low bias since it is performed on n-1 points while K-CV has a much higher bias since it uses $\\frac{(k-1)n}{k} $\n",
        "\n",
        "* K-CV uses less points so has a lower variance and a higher bias\n",
        "\n",
        "* Usually use k =5 and k =10 as these numbers give neither a high bias or a high variance"
      ],
      "metadata": {
        "id": "fqueAApkeInc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 5.1.5 Cross Validation on Clissification\n",
        "\n",
        "* We use a different method than the average of MSE and instead take average of the misclasified points.\n",
        "\n",
        "* We can use multi levels of polynomials and mixed variables. Testing it with the CV and LOOCV methods\n",
        "\n",
        "* We can use LOOCV and CV methods to estimate the best level of flexibility like we did with the others . The minimum from these will be close to the minimum from the real test data(which we will not have)."
      ],
      "metadata": {
        "id": "WdEj_TJGgHEA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 5.2 BootStrap\n",
        "\n",
        "* We can use it first on parameters and their accuracy. However this isn't too valuable since we have.summary() in Python which displays the p values per variable for us.\n",
        "\n",
        "* The more useful feature is it can be used on a wide variety of statistic learning methods which the measure of variability is hard to get.\n",
        "\n",
        "* Normally we would want to continue to produce data and taking more and more measurements to get the most accurate answer. However with bootstrap it uses replacement and we can continue to make unlimited observations like we are making new data.\n",
        "\n",
        "* We select a number n and it picks n random points from our data set. IT does this k types then averages them to produce an accurate result."
      ],
      "metadata": {
        "id": "DqNcqtH7jJqg"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "39kr22ccnTXc"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}