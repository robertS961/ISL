{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# 4.1 Overview of Classification\n",
        "\n",
        "* Classification is the use of qualative variables compared to Regression where we used quanatitive variables.\n",
        "\n",
        "* Classification is actually more common and is important to predict a categroy someonething might fall into like if this gene willl produce cancer or a person will default on their payment.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "ioYsqXPk24PM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 4.2 Why Not Linear Regression\n",
        "\n",
        "* If there is more than 2 response variables that lack similar meaning then it will throw off our values. Like stroke, heart attack, and flu. We could assign\n",
        "1 = stroke ,  2= heart attack, 3= flu. When we use regression it implies some ordering but we could write the same thing with 1 = flu , 2 = heart attack, and 3 = stroke.\n",
        "\n",
        "* Ordering would make sense with data like cold, normal, hot or low, medium, high.\n",
        "\n",
        "> **So we need a natural ordering and feel like the gaps between them are resonable**\n",
        "\n",
        "* If there is two choices then linear regression could be more viable, but it wont provide a meaningful estimate of Pr(X|Y) and can be hard to interpret.\n"
      ],
      "metadata": {
        "id": "_bhHZFx_4wHx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 4.3 Logistic Regression\n",
        "\n",
        "* This will predict the probability of an outcome given some factor. Pr(Defaulting|Balance). Probability of defaulting given the balance which will fall between 0 and 1\n",
        "\n",
        "## 4.3.1 The logistic model\n",
        "* If we try to fit the default|balance model using linear regression we could be given a probability less than 0 and greater than 1 for extremely low and high balances. This doesn't make much sense as we want a meaningful probability.\n",
        "* $log(\\frac{p(x)}{1 - p(x)})  = B_{0} + B_{1}X $ **(4.4) Logit**\n",
        "\n",
        "\n",
        "* $\\frac{p(x)}{1 - p(x)} = e^{B_{0} + B_{1}X} $ **(4.3) ODDS**\n",
        "\n",
        "* $ p(x) = \\frac {e^{B_{0} + B_{1}X}}{1 + e^{B_{0} + B_{1}X}}$ **(4.2) Prob of X**\n",
        "\n",
        "> These formulas better predict the probability, odds, or logit of default or any categorical variable\n",
        "\n",
        "## 4.3.2 Estimating the regression coefficients\n",
        "\n",
        "* We want to use the maximum likehood function which means we want to select $B_{0}$ and $B_{1}$ that plug in 4.2 and give the most accurate $P(x)$.\n",
        "\n",
        "* One unit increase of $B_{1}$ results an increase of $P(x)$ equal to 0.005. Means the probability increased by 0.005 to default.\n",
        "\n",
        "* We can use the z statistic again to reject the null hypothesis. If the the z is larger it implies a significance, just like the t statistic which is used to calculate p. So we can calculate p for $B_{1}$ and see that it is indeed low so can reject null. There is a  relationship between balance and defaulting.\n",
        "\n",
        "## 4.3.3 Making predictions\n",
        "\n",
        "* Use the formula in 4.2 and plug in the values disocvered in 4.3.2. You can also plug in dummy variables from hot enconding that we used in 3.3.1. They will both give us the $P(x)$\n",
        "\n",
        "## 4.3.4 Muliple logical regression\n",
        "* We can rewrite 4.4 as follows: $log(\\frac{p(x)}{1 - p(x)})  = B_{0} + B_{1}X + .... B_{n}X_{n}$ **(4.6)**\n",
        "* We can also rewrite 4.2 as follows: $ p(x) = \\frac {e^{B_{0} + B_{1}X+ .... B_{n}X_{n}}}{1 + e^{B_{0} + B_{1}X + .... B_{n}X_{n}}}$ **(4.7)**\n",
        "\n",
        "* Watch for *confounding* where using less variables can sometimes lead to the wrong result. Given more information can fix the *confounding* problem\n",
        "\n",
        "## 4.3.5 Multinomial logistic regression\n",
        "\n",
        "* Currently have been prediction two results. Default or not default but what if there is more. This is hwere we want to use mulitnomial logistic regression\n",
        "\n",
        "* We select a K as the baseline for the multinomial and do the following:\n",
        "$Pr(Y = K| X = x ) = \\frac {e^{B_{0} + B_{1}X+ .... B_{n}X_{n}}}{1 + \\sum_ {l = 1} ^{K-1} e^{B_{0} + B_{1}X + .... B_{n}X_{n}}}  $\n",
        "\n",
        "* We can chose anything as the baseline and it will not change our final probability however it will change the interpretation.\n",
        "\n"
      ],
      "metadata": {
        "id": "H3m5hTZh7Lb2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 4.4 Generative Models for Classification\n",
        "* Logistical regression we model the conditional distribution response Y given the predictors X. Now there is an alternate approach where we model the distribution of the predictors X seperately in each response to the classes(i.e each value of Y). Then use Bayes to flip it.\n",
        "* Why do this tho?\n",
        "1. When there is significant sperationg between the two classes. It causes logical regression to not work as well\n",
        "1. If the distriubtion of each predictor is normal and the size is quite small. This most liklely will work better\n",
        "1. Can also work with more than 2 response classes like mulitnomial logisitic regression\n",
        "\n",
        "* $f_{k}(x) = Pr(X| Y= k )$ is now our new goal based on modeling X based on Y = k. **(4.14)** Called the density function\n",
        "* $Pr(Y= k | X= x ) = \\frac{\\pi_{k}f_{k}(x)}{\\sum_{l = 1}^{K}\\pi_{l}f_{l}(x)} $ **(4.15)** by using Bayes\n",
        "\n",
        "* Now must discover what $\\pi_{k}$ is and it is relatively easy to compute. However the density function is much harder.\n",
        "\n",
        "## 4.4.1 Linear discriminant analysis for p = 1\n",
        "\n",
        "* We must assume the data is normal first. Second we assume a shared variance across all the K classes.\n",
        "\n",
        "* Calculate the density function as $f_{k}(x) = \\frac{1}{\\sqrt{2\\pi\\sigma_{k}}} exp(-\\frac{(x-\\mu_{k})^{2}}{2\\sigma_{k}^{2}})$ **(4.16)**\n",
        "* You then plug this function into (4.15)\n",
        "* Then take the log of both sides and put in estimates for the variance, mean, and $\\pi_{k}$\n",
        "* This will give us the Linear discriminant analysis. The Linear discriminant analysis approximates Bayes from (4.16) into (4.15)\n",
        "* Bays decision point is where all the classes $\\delta_{1}$ = $\\delta_{2}$\n",
        "\n",
        "## 4.4.2 Linear discriminant anlaysis for p > 1\n",
        "* We utilize a new equation similar to the above to adapt for muliple classes. We need to have a mulitvariate normal. Basically means each predictor follows a 1d normal curve with some correlation between predictors. Along with a common covariance matrix and class specific means vectors.\n",
        "\n",
        "* Once predict we should use a confusion matrix to see how the results turned out. Rather than the percent error as it can be misleading. Especially if the null error is already low(like if we predicted everything to be the same)\n",
        "\n",
        "* We can alter bayes threshold which might increase the error but alter specific variables errors to be more accurate. This can be important for specific companies or goals.\n",
        "\n",
        "* How to decide tho which threshold value is best? We will use ROC curve. We want the ROC curve to hug the top left side of the graph and get to as close to 1.0 as possible\n",
        "\n",
        "## 4.4.3 Quadratic discriminant anlyaysis\n",
        "* Like LDA that it assumes each class is drawn from a normal distribution and still has class specific mean.However it differs as it does assume that each class has its own covariance matrix. This will give a quadradic approach for all the classes when predicting bayes. Unlike the linear from LDA.\n",
        "\n",
        "* We want to pick LDA when we want a more rigid approach, limiting variance. We would want to limit variance when there is a small number of predictors.\n",
        "\n",
        "* We would want to pick QDA when we want to be more flexible and have a lot more predictors. This way we can care less about the variance. And if each class has a large variance difference the assumption from LDA can throw the prediction off.\n",
        "\n",
        "## 4.4.4 Niave Bayes\n",
        "* Previous we assumed normal distribution for each K class and K means, for LDA we assumed 1 variance for K classes while QDA we assumed K variances. We were able to plug these in and solve for 1 density function.\n",
        "\n",
        "* Naive Bayes will assume within each class p predictors are independent. Allows us to muliply all the K desnity functions to discover the overall density function.\n",
        "\n",
        "* This eliminates a ton of complexity since we don't have to worry about classes desnity functions interaction or relying on other classes.\n",
        "\n",
        "* Works well with a ton of data or when n is not relatively large to p.\n",
        "\n",
        "* Naive Bayes Density function will then be : $f_{k}(x) = f_{k1}(x_{1}) f_{k2}(x_{2}) + ..... + f_{kn}(x_{n})$ **(4.29)**\n",
        "\n",
        "* Then plug this into (4.15) to get: $Pr(Y= k | X= x ) = \\frac{\\pi_{k} f_{k1}(x_{1}) f_{k2}(x_{2}) + ..... + f_{kn}(x_{n})}{\\sum_{l = 1}^{K}\\pi_{l} f_{l1}(x_{1}) f_{l2}(x_{2}) + ..... + f_{ln}(x_{n})} $ **(4.30)**\n",
        "\n",
        "* To predict the one dimensional density function $f_{kj}$ we can use a few options\n",
        ">1. IF $X_{j}$ is quantitative We can assume within each class the jth predictor is drawn from a normal distirubtion. Like QDA but with an addition assumption that is the class specific covariance matrix is diagonal.\n",
        ">1. If $X_{j}$ is quantitative we also can use a kernel density estimator which is just a smoothed version of a histogram. We make a historgram for $j_{th}$ predictor of that $k_{th}$ class. Then we estimate the density function $f_{kj}(x_{j})$ as a fraction of training data in the $k_{th}$ class that belong to the same bin as $x_{j}$.\n",
        ">1. IF $X_{j}$ is qualitative then simply count the porportion of each class responding to the $j_{th}$ predictor.\n",
        "* We can also reduce the threshold for error on niave bayes like we did with LDA and QDA.\n",
        "\n",
        "* Expect the greatest payoff from niave bayes when we want to reduce variance. So when p is large or n is small.\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "_7ZnwCtyB_tf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 4.5 Comparison of Classification Methods\n",
        "\n",
        "## 4.5.1 An analytical comparison\n",
        "* LDA is a special case of QDA where all the covariance matrix are equal\n",
        "* LDA is a special case of niave base where it assumes features are normally distributed within each class covariance matrix, while niave assumes its independence of this feature\n",
        "* QDA and niave bayes are not a special case for each other. But if there is a lot of interaction between predictors then one should choose QDA\n",
        "* Based on the formulas we except logistic regression to outperform LDA when each class is not  normally distributed. And LDA to out perform logistic regression when each class is normall distributed.\n",
        "* We can also consider K nearest neighbors. Where we located the closed K points and decided the class based on the plurality of these points.\n",
        ">1. Can expect it outperform LDA and logistic regression when the decision boundary is parametric and not linear.\n",
        ">1. In order for KNN to be accurate it needs a lot of classification. A big n for each P. IT will not work if P is large and N is small. Thus Knn will be parametric and limit bias but increase variance.\n",
        ">1. QDA can be prefered when n is modest or p is not very small. Since QDA is also quadratic and requires less data. QDA can take adv of a nonlinear approach using quadradics and take a parametric approach.\n",
        ">1. Knn does not tell us which predictors are import. Slight trade off.\n",
        "\n",
        "## 4.5.2 An emperical comparison\n",
        "* **(LINEAR)** We want LDA and Logistic Regression when our decision line is linear. However we can only pick LDA if it has a standard normal distribution. IF every predictor is not correlated than bayes might do the best. IF there is strong correlation then look to LDA and logistric regression. IF it isnt normal then def logistic regression.\n",
        "* **(NonLinear)** QDA does extremely well with non linear and can deal with correlation since it use class dependent co variance. Bayes does not do well with this since it assumes independence per predictor.Knn can also battle with these two if the correct K is choosen. If the wrong K is picked then it can lead to terrible results.\n",
        "\n",
        "* We can add more parameters to Linear regression and LDA model like $n^{2}, n^{4}$ etc. Just like we did with linear regression.\n",
        "\n",
        "*Take away* : Logistic Regression and LDA for linear boundaries. If it the parameters aren't normally distriubted then LR else LDA. For more parametric boundaries use QDA and Bayes. If there is a strong correlation between variables use QDA, if there isnt then Bayes. If it isnt normal then use Bayes. Lastly can considered KNN but need the right choice for K.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "UYmFQlxasQmh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#4.6 Generalized Linear Models\n",
        "## 4.6.1 Linear regression on the bike share model\n",
        "* Big errors as the scatterplots look appealing with most riders riding at peak hours and peak months\n",
        "* However there is negative bikers at certain hours and months which is impossible\n",
        "* Also the variance and means grow extremely high together and when the mean is low the variance is also high. This is odd and shows possible problems with this approach\n",
        "\n",
        "## 4.6.2 Poisson regression model\n",
        "* This is our fix to the above problem as it is only used in counting where there can be non negative values\n",
        "* Method uses expected value which is variance and assumes to it be the mean. Called this $λ = E(Y)$ Where $E(Y)$ is the expected value, variance and mean. We then expect this function to flucate based on the hours, months etc. Covariates based on predictors.\n",
        "* Interpretation - must remember to use exp so $e^{data}$. Thus $e^{x}$ can never reach 0.\n",
        "* Linear regression can't handle a dynamic mean, variance relationship. In linear regression it takes on a constant, while in poisson it is assumed the mean equals the variance for that given hour.\n",
        "* Lastly there can never be any nonnegative values which we would want from a count bike share info\n",
        "\n",
        "## 4.6.3 Generalized linear models in greater generality\n",
        "* Linear Regression comes from a Gaussian distribution, Logistic regression comes from bernoulli distribution, and poisson regression comes from poisson distribution.\n",
        "* Each approach models the mean of Y from the predictors of X. All 3 equations can be written as a link function since they come from a greater know class called the exponential family.\n",
        "\n",
        "* Formula is to perform a regression by modeling the response Y as coming from one of members in exponential family. Then transform the mean of the response so it takes on the form of a linear function of its predictors. **Formula : Take Y and apply a regression technique on it to model it linearly from its predictors.**\n",
        "\n",
        "* Others not included here are Gamma regression and negative binomial regression that belong to the exponential family."
      ],
      "metadata": {
        "id": "iuFvjtNQ7hkJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 4.7 Lab Logistic Regression, LDA, QDA ,  KNN\n",
        "* This will continue on the LabC4 notebook in the GitHub!"
      ],
      "metadata": {
        "id": "XxTzDvG8E4oU"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "XIrG7IqgFjbP"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}