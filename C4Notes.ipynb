{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# 4.1 Overview of Classification\n",
        "\n",
        "* Classification is the use of qualative variables compared to Regression where we used quanatitive variables.\n",
        "\n",
        "* Classification is actually more common and is important to predict a categroy someonething might fall into like if this gene willl produce cancer or a person will default on their payment.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "ioYsqXPk24PM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 4.2 Why Not Linear Regression\n",
        "\n",
        "* If there is more than 2 response variables that lack similar meaning then it will throw off our values. Like stroke, heart attack, and flu. We could assign\n",
        "1 = stroke ,  2= heart attack, 3= flu. When we use regression it implies some ordering but we could write the same thing with 1 = flu , 2 = heart attack, and 3 = stroke.\n",
        "\n",
        "* Ordering would make sense with data like cold, normal, hot or low, medium, high.\n",
        "\n",
        "> **So we need a natural ordering and feel like the gaps between them are resonable**\n",
        "\n",
        "* If there is two choices then linear regression could be more viable, but it wont provide a meaningful estimate of Pr(X|Y) and can be hard to interpret.\n"
      ],
      "metadata": {
        "id": "_bhHZFx_4wHx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 4.3 Logistic Regression\n",
        "\n",
        "* This will predict the probability of an outcome given some factor. Pr(Defaulting|Balance). Probability of defaulting given the balance which will fall between 0 and 1\n",
        "\n",
        "## 4.3.1 The logistic model\n",
        "* If we try to fit the default|balance model using linear regression we could be given a probability less than 0 and greater than 1 for extremely low and high balances. This doesn't make much sense as we want a meaningful probability.\n",
        "* $log(\\frac{p(x)}{1 - p(x)})  = B_{0} + B_{1}X $ **(4.4) Logit**\n",
        "\n",
        "\n",
        "* $\\frac{p(x)}{1 - p(x)} = e^{B_{0} + B_{1}X} $ **(4.3) ODDS**\n",
        "\n",
        "* $ p(x) = \\frac {e^{B_{0} + B_{1}X}}{1 + e^{B_{0} + B_{1}X}}$ **(4.2) Prob of X**\n",
        "\n",
        "> These formulas better predict the probability, odds, or logit of default or any categorical variable\n",
        "\n",
        "## 4.3.2 Estimating the regression coefficients\n",
        "\n",
        "* We want to use the maximum likehood function which means we want to select $B_{0}$ and $B_{1}$ that plug in 4.2 and give the most accurate $P(x)$.\n",
        "\n",
        "* One unit increase of $B_{1}$ results an increase of $P(x)$ equal to 0.005. Means the probability increased by 0.005 to default.\n",
        "\n",
        "* We can use the z statistic again to reject the null hypothesis. If the the z is larger it implies a significance, just like the t statistic which is used to calculate p. So we can calculate p for $B_{1}$ and see that it is indeed low so can reject null. There is a  relationship between balance and defaulting.\n",
        "\n",
        "## 4.3.3 Making predictions\n",
        "\n",
        "* Use the formula in 4.2 and plug in the values disocvered in 4.3.2. You can also plug in dummy variables from hot enconding that we used in 3.3.1. They will both give us the $P(x)$\n",
        "\n",
        "## 4.3.4 Muliple logical regression\n",
        "* We can rewrite 4.4 as follows: $log(\\frac{p(x)}{1 - p(x)})  = B_{0} + B_{1}X + .... B_{n}X_{n}$ **(4.6)**\n",
        "* We can also rewrite 4.2 as follows: $ p(x) = \\frac {e^{B_{0} + B_{1}X+ .... B_{n}X_{n}}}{1 + e^{B_{0} + B_{1}X + .... B_{n}X_{n}}}$ **(4.7)**\n",
        "\n",
        "* Watch for *confounding* where using less variables can sometimes lead to the wrong result. Given more information can fix the *confounding* problem\n",
        "\n",
        "## 4.3.5 Multinomial logistic regression\n",
        "\n",
        "* Currently have been prediction two results. Default or not default but what if there is more. This is hwere we want to use mulitnomial logistic regression\n",
        "\n",
        "* We select a K as the baseline for the multinomial and do the following:\n",
        "$Pr(Y = K| X = x ) = \\frac {e^{B_{0} + B_{1}X+ .... B_{n}X_{n}}}{1 + \\sum_ {l = 1} ^{K-1} e^{B_{0} + B_{1}X + .... B_{n}X_{n}}}  $\n",
        "\n",
        "* We can chose anything as the baseline and it will not change our final probability however it will change the interpretation.\n",
        "\n"
      ],
      "metadata": {
        "id": "H3m5hTZh7Lb2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 4.4 Generative Models for Classification\n",
        "* Logistical regression we model the conditional distribution response Y given the predictors X. Now there is an alternate approach where we model the distribution of the predictors X seperately in each response to the classes(i.e each value of Y). Then use Bayes to flip it.\n",
        "* Why do this tho?\n",
        "1. When there is significant sperationg between the two classes. It causes logical regression to not work as well\n",
        "1. If the distriubtion of each predictor is normal and the size is quite small. This most liklely will work better\n",
        "1. Can also work with more than 2 response classes like mulitnomial logisitic regression\n",
        "\n",
        "* $f_{k}(x) = Pr(X| Y= k )$ is now our new goal based on modeling X based on Y = k. **(4.14)** Called the density function\n",
        "* $Pr(Y= k | X= x ) = \\frac{\\pi_{k}f_{k}(x)}{\\sum_{l = 1}^{K}\\pi_{l}f_{l}(x)} $ **(4.15)** by using Bayes\n",
        "\n",
        "* Now must discover what $\\pi_{k}$ is and it is relatively easy to compute. However the density function is much harder.\n",
        "\n",
        "## 4.4.1 Linear discriminant analysis for p = 1\n",
        "\n",
        "* We must assume the data is normal first. Second we assume a shared variance across all the K classes.\n",
        "\n",
        "* Calculate the density function as $f_{k}(x) = \\frac{1}{\\sqrt{2\\pi\\sigma_{k}}} exp(-\\frac{(x-\\mu_{k})^{2}}{2\\sigma_{k}^{2}})$ **(4.16)**\n",
        "* You then plug this function into (4.15)\n",
        "* Then take the log of both sides and put in estimates for the variance, mean, and $\\pi_{k}$\n",
        "* This will give us the Linear discriminant analysis. The Linear discriminant analysis approximates Bayes from (4.16) into (4.15)\n",
        "* Bays decision point is where all the classes $\\delta_{1}$ = $\\delta_{2}$\n",
        "\n",
        "## 4.4.2 Linear discriminant anlaysis for p > 1\n",
        "* We utilize a new equation similar to the above to adapt for muliple classes. We need to have a mulitvariate normal. Basically means each predictor follows a 1d normal curve with some correlation between predictors. Along with a common covariance matrix and class specific means vectors.\n",
        "\n",
        "* Once predict we should use a confusion matrix to see how the results turned out. Rather than the percent error as it can be misleading. Especially if the null error is already low(like if we predicted everything to be the same)\n",
        "\n",
        "* We can alter bayes threshold which might increase the error but alter specific variables errors to be more accurate. This can be important for specific companies or goals.\n",
        "\n",
        "* How to decide tho which threshold value is best? We will use ROC curve. We want the ROC curve to hug the top left side of the graph and get to as close to 1.0 as possible\n",
        "\n",
        "## 4.4.3 Quadratic discriminant anlyaysis\n",
        "* Like LDA that it assumes each class is drawn from a normal distribution and still has class specific mean.However it differs as it does assume that each class has its own covariance matrix. This will give a quadradic approach for all the classes when predicting bayes. Unlike the linear from LDA.\n",
        "\n",
        "* We want to pick LDA when we want a more rigid approach, limiting variance. We would want to limit variance when there is a small number of predictors.\n",
        "\n",
        "* We would want to pick QDA when we want to be more flexible and have a lot more predictors. This way we can care less about the variance. And if each class has a large variance difference the assumption from LDA can throw the prediction off.\n",
        "\n",
        "## 4.4.4 Niave Bayes\n",
        "* Previous we assumed normal distribution for each K class and K means, for LDA we assumed 1 variance for K classes while QDA we assumed K variances. We were able to plug these in and solve for 1 density function.\n",
        "\n",
        "* Naive Bayes will assume within each class p predictors are independent. Allows us to muliply all the K desnity functions to discover the overall density function.\n",
        "\n",
        "* This eliminates a ton of complexity since we don't have to worry about classes desnity functions interaction or relying on other classes.\n",
        "\n",
        "* Works well with a ton of data or when n is not relatively large to p.\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "_7ZnwCtyB_tf"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "UYmFQlxasQmh"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}