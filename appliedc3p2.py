# -*- coding: utf-8 -*-
"""AppliedC3P2.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1V6V1o3bb97qcMmiGI4M_HcLqv6oVsOLR
"""

import numpy as np
import pandas as pd
import seaborn as sns; sns.set()
import matplotlib.pyplot as plt
from mpl_toolkits.mplot3d import Axes3D
import statsmodels.api as sm
import statsmodels.formula.api as smf
import patsy
from scipy import stats
from sklearn import datasets
from IPython.display import display, HTML

pip install ISLP

from ISLP import load_data
from ISLP.models import(ModelSpec as MS, summarize, poly)
Carseats = load_data('Carseats')

"""# Question 10

This question should be answered using the Carseats data set.

(a) Fit a multiple regression model to predict Sales using Price,
Urban, and US.
"""

Carseats.info()

Carseats.dropna()
Carseats.info()

Carseats.describe()

datatypes = {
    'quant': ['Sales', 'CompPrice','Income','Advertising','Population','Price','Age','Education'],
    'qual': ['ShelveLoc', 'Urban','US']
}
quants = Carseats[datatypes['quant']].astype('float')
carseats_df = pd.concat([quants,Carseats[datatypes['qual']]], axis = 1)
carseats_df.head()

f = 'Sales ~ Price + C(Urban) + C(US)'
y,X = patsy.dmatrices(f, carseats_df, return_type ='dataframe' )
model = sm.OLS(y,X).fit()
print(model.summary())
y_pred = np.array(model.predict(X))

Carseats.head()

c = Carseats.copy()
ShelveLoc = {'Bad':0, 'Medium' :1, 'Good':2}
yesNo = {'Yes':1, 'No' :0}
c['ShelveLoc'] = c['ShelveLoc'].map(ShelveLoc)
c['US'] = c['US'].map(yesNo)
c['Urban'] = c['Urban'].map(yesNo)
c.head()

cat = ['US', 'ShelveLoc', 'Urban']
c['Urban'] = c['Urban'].astype(int)
c['US'] = c['US'].astype(int)
c['ShelveLoc'] = c['ShelveLoc'].astype(int)
c.dtypes

newF = 'Sales ~ Price + Urban + US'
rTry2 = smf.ols(newF, data = c).fit()
rTry2.summary()

"""# **(b) Provide an interpretation of each coefficient in the model. Be careful—some of the variables in the model are qualitative!**

1)Price - The more expensive the object the less it will sell for. This is a solid variable since the P value is low.
2)Urban - Means if the location is Urban or not. If it is urban then it will decrease the sale since coef is negative. However the p value is low, so we need to investiage this variable into more consideration.
3)US - if the object is from the US then it will increase the sale since the coef is positive. This has a low p value so it is likely relevant.

# **(c) Write out the model in equation form, being careful to handle the qualitative variables properly.**

Sales = 13.0435 + Price X -0.0545 + Urban X -0.219 + US X 1.2006

# **(d) For which of the predictors can you reject the null hypothesis H0 : βj = 0?**
The F statistic is high so we can reject the null. The p value is high for Urban. So we can reject it for US and Price.
"""

newnewF = 'Sales ~ US + Price'
r = smf.ols(newnewF, data = c).fit()
r.summary()

"""# **(e) On the basis of your response to the previous question, fit a smaller model that only uses the predictors for which there is evidence of association with the outcome.**

The R^2 value is the same so the model hasn't approved its accuracy. But it shows that Urban was not relevant to the models accuracy. Therefore it is better that it is removed.

# **(f) How well do the models in (a) and (e) ft the data?**
Answered above in (e)

# **(g) Using the model from (e), obtain 95 % confdence intervals for the coefcient(s).**

We can see the itervals from the summary.

Price is [-0.065, -0.044]


US is [-0.692, 1.708]

# **(f) Is there evidence of outliers or high leverage observations in the model from (e)?**
"""

plt.figure(figsize = (12,8))
plt.ylim(-15,15)
sns.regplot(x = r.fittedvalues, y = r.resid, lowess = True)
plt.axhline(y = 0, linewidth = 1.0, linestyle = 'dashed', color = 'red')
plt.xlabel('Fittedvalues')
plt.ylabel('Residuals')
plt.title('Residuals X Fittedvalues')

"""The residuals seems to be relatively close together in a linear fashion with limited outliers!"""

plt.figure(figsize=(12,8))
infl = r.get_influence()
plt.ylim(-.25,.25)
sns.regplot(x = np.arange(X.shape[0]), y =infl.hat_matrix_diag, lowess = True)
plt.xlabel("X values")
plt.ylabel("Leverage")
plt.title("Leverage for x Values")

"""We can see from the above graph there is very little leverage. The data from this data set is relatively accurate in terms of outliers and high leverage.

### Question 11
 In this problem we will investigate the t-statistic for the null hypothesis H0 : β = 0 in simple linear regression without an intercept. To
begin, we generate a predictor x and a response y as follows.



```
rng = np.random.default_rng(1)
x = rng.normal(size=100)
y = 2 * x + rng.normal(size=100)
```

(a) Perform a simple linear regression of y onto x, without an intercept. Report the coefcient estimate βˆ, the standard error of
this coefcient estimate, and the t-statistic and p-value associated with the null hypothesis H0 : β = 0. Comment on these
results. (You can perform regression without an intercept using
the keywords argument intercept=False to ModelSpec().)
"""

rng = np.random.default_rng(1)
x = rng.normal(size=100)
y = 2 * x + rng.normal(size=100)

results1 = sm.OLS(x,y).fit()
results1.summary()

"""# Results
Coef is .3757 and the std err is 0.22 with a low p value. The t statistic is 16.898.

**(b) Now perform a simple linear regression of x onto y without an intercept, and report the coefcient estimate, its standard error, and the corresponding t-statistic and p-values associated with the null hypothesis H0 : β = 0. Comment on these results**
"""

results2 = sm.OLS(y,x).fit()
results2.summary()

"""**Observations**
>
The coef is much large being 1.97 and the std err is also large at .117. The t statistic is the same tho and the p value is the same. The R^2 for both is the same also. WE can reject the null hypothesis for both

# (c) What is the relationship between the results obtained in (a) and (b)?
>
We can see that they both have the same t and p value, but different coef and std error.

# (d) Show algebraically, and confrm numerically in R, that the t-statistic can be written a
Did it on scratch paper

# (e) Using the results from (d), argue that the t-statistic for the regression of y onto x is the same as the t-statistic for the regression of x onto y.

The equation will not change if you switch x and y. Therefore the result will stay the same. It is mirrored for x and y

# (f) In R, show that when regression is performed with an intercept, the t-statistic for H0 : β1 = 0 is the same for the regression of y onto x as it is for the regression of x onto y.
"""

data = pd.DataFrame({'X':x, 'y': y})
r1= smf.ols('X ~ y', data = data ).fit()
print(r1.summary())
r2=smf.ols('y ~ X', data = data).fit()
print(r2.summary())

"""You can see they have the same R^2 and Adj R along with the t value and p value!

# Question 12

# This problem involves simple linear regression without an intercept.

(a) Recall that the coefficient estimate βˆ for the linear regression of Y onto X without an intercept is given by (3.38). Under what circumstance is the coefficient estimate for the regression of X onto Y the same as the coefficient estimate for the regression of Y onto X

Answer: When the summation of Xi equals the summation of Yi.

(b) Generate an example in Python with n = 100 observations in which the coefficient estimate for the regression of X onto Y is different from the coefficient estimate for the regression of Y onto X.
"""

np.random.seed(2)
X = np.random.normal(size = 100)
y = 1.2 * X
df = pd.DataFrame({'X': X, 'y':y})
sns.scatterplot(x = X, y= y)

r1 = sm.OLS(X,y).fit()
r2 = sm.OLS(y,X).fit()
print(r1.summary(), r2.summary())

"""The coef are different as X onto y is .833 and y onto X is 1.2. It is shown that the p values are the same as expected but the t values differ

(c) Generate an example in Python with n = 100 observations in
which the coefcient estimate for the regression of X onto Y is
the same as the coefcient estimate for the regression of Y onto
X.
"""

X = np.random.normal(size = 100)
y = X
r1 = sm.OLS(X,y).fit()
r2 = sm.OLS(y,X).fit()
print(r1.summary(), r2.summary())

"""Hence as predict when X=y then their coefficents are equal as predicted above based on the 3.38 formula for B hat.

# Question 13

In this exercise you will create some simulated data and will ft simple
linear regression models to it. Make sure to use the default random
number generator with seed set to 1 prior to starting part (a) to
ensure consistent results.

(a) Using the normal() method of your random number generator,
create a vector, x, containing 100 observations drawn from a
N(0, 1) distribution. This represents a feature, X.
"""

np.random.seed(1)
X = np.random.normal(loc = 0, scale = 1, size = 100)

"""(b) Using the normal() method, create a vector, eps, containing 100
observations drawn from a N(0, 0.25) distribution—a normal
distribution with mean zero and variance 0.25
"""

eps = np.random.normal(loc = 0, scale =0.25, size = 100)

"""(c) Using x and eps, generate a vector y according to the model


Y = −1 + 0.5X + eps. (3.39)


What is the length of the vector y? What are the values of β0
and β1 in this linear model?

Answer: Bo is -1 which is the intercept. B1 is 0.5.
"""

Y = -1 + 0.5 * X + eps

Y.size
#This is the length of Y

"""(d) Create a scatterplot displaying the relationship between x and
y. Comment on what you observe.
"""

X = np.random.normal(loc = 0, scale = 1, size = 100)
eps = np.random.normal(loc = 0, scale =0.25, size = 100)
Y = -1 + 0.5 * X + eps
plt.figure(figsize=(12,8))
ax = sns.scatterplot(x = X, y = Y)
plt.xlabel('X')
plt.ylabel('Y')
plt.title('X vs Y')

"""The relationship is rather linear as X grows so does Y with a similar term coef. It appears the coef is around 0.5 and the intercept is -1.0

(e) Fit a least squares linear model to predict y using x. Comment
on the model obtained. How do βˆ0 and βˆ1 compare to β0 and
β1?
"""

X = sm.add_constant(X)
results = sm.OLS(X,Y).fit()

results.params

"""The test B0 is similar to the BO from the graph only different by -.2.

Likewise the test B1 is relatively close to B1 from the graph. Also differing by .2
"""

X = np.random.normal(loc = 0, scale = 1, size = 100)
eps = np.random.normal(loc = 0, scale =0.25, size = 100)
Y = -1 + 0.5 * X + eps
model = smf.ols('Y ~ X', data = pd.DataFrame({'x': X, 'y': Y})).fit()
model.summary()

"""(f) Display the least squares line on the scatterplot obtained in (d).
Draw the population regression line on the plot, in a diferent
color. Use the legend() method of the axes to create an appropriate legend.
"""

y_predict = model.predict()
y_regression = -1 + 0.5 * X

plt.figure(figsize= (12,8))
ax = sns.scatterplot(x= X, y = Y)
plt.xlabel('X')
plt.ylabel('y')
plt.title("X vs Y")

ax.plot(X, y_predict, color = 'r')
ax.plot(X, y_regression, color = 'y')

ax.legend(['Standard', 'Least Square', 'Regression'])

"""(g) Now fit a polynomial regression model that predicts y using x
and x2. Is there evidence that the quadratic term improves the
model ft? Explain your answer.
"""

data = pd.DataFrame({'x': X, 'y':Y})
f1 = 'Y ~ X'
f2 = 'Y~ X + np.power(X,2)'
r1 = smf.ols(f1, data = data).fit()
r2 = smf.ols(f2, data = data).fit()
print(r1.summary(), r2.summary())

y_predict = r1.predict()
y_pow_predict = r2.predict()
y_regression = -1 + 0.5 * X

plt.figure(figsize= (12,8))
ax = sns.scatterplot(x= X, y = Y)
plt.xlabel('X')
plt.ylabel('y')
plt.title("X vs Y")

ax.plot(X, y_predict, color = 'r')
ax.plot(X, y_regression, color = 'y')
ax.plot(X, y_pow_predict, color = 'g')

ax.legend(['Standard', 'Least Square', 'Regression', 'Power2Regression'])

"""The pow seems to have a high p value making it insignificant along with not increasing the R^2 value for the fit. Also plotting it shows no real change or significance!

(h) Repeat (a)–(f) after modifying the data generation process in
such a way that there is less noise in the data. The model (3.39)
should remain the same. You can do this by decreasing the variance of the normal distribution used to generate the error term
" in (b). Describe your results.
"""

X = np.random.normal(loc = 0, scale = 1, size = 100 )
eps = np.random.normal(loc = 0, scale =.025, size = 100)
Y = -1 + .5*X + eps
#size of y
print("Size of Y: ", Y.size)

data = pd.DataFrame({'x':X, 'y':Y})

results = smf.ols('Y ~ X', data = data).fit()
print("These are the results: ", results.summary())

results2 = smf.ols('Y ~ X + np.power(X,2)', data = data).fit()
print("These are the results for power2 regression", results2.summary())

plt.figure(figsize= (12,8))
ax = sns.scatterplot(x= X, y = Y)
plt.xlabel('X')
plt.ylabel('Y')
plt.title('X vs Y')

ax.plot(X, results.predict(), color = 'r')
ax.plot(X, results2.predict(), color = 'y')
ax.plot(X, -1 + 0.5*X, color = 'b')
ax.legend(['Standard', 'Least Squares', 'Least Squares with power2', 'Regression'])

"""Reducing this variance has resulted in a higher R^2 value. It has risen from .81 to .9998. We can also see this on our graph as our least squared lines better fit the data!

(i) Repeat (a)–(f) after modifying the data generation process in
such a way that there is more noise in the data. The model
(3.39) should remain the same. You can do this by increasing
the variance of the normal distribution used to generate the
error term " in (b). Describe your results.
"""

X = np.random.normal(loc = 0, scale = 1, size = 100 )
eps = np.random.normal(loc = 0, scale =.95, size = 100)
Y = -1 + (.5*X) + eps
#size of y
print("Size of Y: ", Y.size)

data = pd.DataFrame({'x': X,'y':Y})
r1 = smf.ols('Y~X', data = data).fit()
r2 = smf.ols('Y~X + np.square(X)', data = data).fit()
print(r1.summary(), r2.summary())

plt.figure(figsize = (14,8))
ax = sns.scatterplot(x = X, y = Y)
plt.xlabel('x')
plt.ylabel('y')
plt.title('x vs y')

ax.plot(X, r1.predict(), color = 'r')
ax.plot(X, r2.predict(), color = 'y')
ax.plot(X, -1 + (.5 * X), color = 'b')
ax.legend(['Standard', 'LS' ,'LS with pow', 'Regres'])

"""Our R^2 value went down to around .2. This indicates that our least square estimates do not fit the data very well. As we can see from our three lines this is indeed true. So more variance than less fit regression!

(j) What are the confdence intervals for β0 and β1 based on the
original data set, the noisier data set, and the less noisy data
set? Comment on your results.

Noise is :    

Intercept -1.280      -0.765
X          0.338       0.737

Less Noise :     

Intercept  -1.088      -0.958
X           0.460       0.552

Original :     

Intercept	-1.085	-0.980
X	         0.461	0.553

The more variance the larger the interval of confidence, the less variance then the tighter the interval. We can see this from the above based on the intercept and X value.

# Question 14
This problem focuses on the collinearity problem.

(a) Perform the following commands in Python:
"""

rng = np.random.default_rng(10)
x1 = rng.uniform(0, 1, size=100)
x2 = 0.5 * x1 + rng.normal(size=100) / 10
y = 2 + 2 * x1 + 0.3 * x2 + rng.normal(size=100)

"""The last line corresponds to creating a linear model in which y is
a function of x1 and x2. Write out the form of the linear model.
What are the regression coefcients?

Bo = 2

B1 = 2

B2 = 0.3

B0 + B1x1 + B2x2 = Y

(b) What is the correlation between x1 and x2? Create a scatterplot
displaying the relationship between the variables.
"""

data = pd.DataFrame({'x1': x1, 'x2': x2, 'y':y})
plt.figure(figsize=(12,8))
ax = sns.scatterplot(x = x1, y = x2, data = data)
plt.xlabel('x1')
plt.ylabel('x2')
plt.title('x1 vs x2')
print(np.corrcoef(x1,x2)[0][1])
print(data.corr())

"""The variables x1 and x2 are strongly correlated. We can see this in two ways.

1) The graph where the points are similar

2)The correlation coef is high at .78

(c) Using this data, fit a least squares regression to predict y using
x1 and x2. Describe the results obtained. What are βˆ0, βˆ1, and
βˆ2? How do these relate to the true β0, β1, and β2? Can you
reject the null hypothesis H0 : β1 = 0? How about the null
hypothesis H0 : β2 = 0?
"""

results = smf.ols('y ~ x1 + x2', data = data).fit()
results.summary()

"""The results show that B0 is 1.9579 which is close to 2 are actual value!

B1 is 1.62 which is close to the actual value at 2.

B2 is .9428 which is a little far away from 0.3. It is the least acccurate.

We can most certainly reject the null hypothesis as a whole since our f statistic is high.

We can reject the null hypothesis for x1 since our P value is low. However our p value is a little high for x2, so unsure if we can reject it.

(d) Now fit a least squares regression to predict y using only x1.
Comment on your results. Can you reject the null hypothesis
H0 : β1 = 0?
"""

results = smf.ols('y ~ x1', data= data).fit()
results.summary()

"""We can most certianly reject the null hypothesis as our F statistic is high and our p value is extremely low.

Our B0 and B1 values are also more accurate now!However our R value did decrease by .01 which isnt very significant.

(e) Now ft a least squares regression to predict y using only x2.
Comment on your results. Can you reject the null hypothesis
H0 : β1 = 0?
"""

results = smf.ols('y ~ x2', data = data ).fit()
results.summary()

"""We would once again reject the null hypothesis since our F statistic is high and p value for x2 is low. However our R squared did significantly decrease by .06. This may be less accurate

(f) Do the results obtained in (c)–(e) contradict each other? Explain
your answer.

Yes they do. In C we said that X2 was not significant since it had a high p value. However when tested alone its p value significantly dropped. This shows there is some correlation between the two variables which we predicted in b.

(g) Suppose we obtain one additional observation, which was unfortunately mismeasured. We use the function np.concatenate() to np.concaadd this additional observation to each of x1, x2 and y.
"""

x1 = np.concatenate([x1, [0.1]])
x2 = np.concatenate([x2, [0.8]])
y = np.concatenate([y, [6]])

"""Re-ft the linear models from (c) to (e) using this new data. What
efect does this new observation have on the each of the models?
In each model, is this observation an outlier? A high-leverage
point? Both? Explain your answers.
"""

data = pd.DataFrame({'x1': x1, 'x2' : x2, 'y' : y})
r1 = smf.ols('y ~ x1 + x2', data = data).fit()
r2 = smf.ols('y ~ x1', data = data).fit()
r3 = smf.ols('y ~ x2', data = data).fit()
print(r1.summary(), r2.summary(), r3.summary())

sns.scatterplot(x = x1, y = x2)

"""We can see the new point is an outlier as it is extremely far away from the linear trend. This would make sense why our B2 value is so far off in r3."""

sns.scatterplot(x = x1, y = y)

sns.scatterplot(x = x2, y= y)

"""It seems our point in X1 is normal as it doesn't go outside the range. However our point in y is an outlier based on the 1st and 2nd scatter plot. We can see how much futher away it is from the rest of the data. Our point in x2 is high leverage as we don have any x2 values in that range. This explains why our p values were kept low, and our F statistic was high. Yet our B1 and B2 coef were further off than before.

# Question 15

This problem involves the Boston data set, which we saw in the lab
for this chapter. We will now try to predict per capita crime rate
using the other variables in this data set. In other words, per capita
crime rate is the response, and the other variables are the predictors.

(a) For each predictor, fit a simple linear regression model to predict
the response. Describe your results. In which of the models is
there a statistically signifcant association between the predictor
and the response? Create some plots to back up your assertions.
"""

Boston = load_data("Boston")

Boston.head()

Boston.info()

Boston.dropna()

Boston.info()

cols = ['zn', 'indus','chas','nox','rm','age','dis','rad','tax', 'ptratio', 'lstat', 'medv']
y = Boston['crim']
for col in cols:
  r= sm.OLS(y, Boston[col]).fit()
  print(r.summary())
  sns.scatterplot(x = Boston[col], y =y)
  plt.show()

"""Looks like everything is signifcant except for ZN and chas based on our p values and graphs!

(b) Fit a multiple regression model to predict the response using
all of the predictors. Describe your results. For which predictors
can we reject the null hypothesis H0 : βj = 0?
"""

predictors = " + ".join(cols)
rOverall = sm.OLS(y, Boston[cols]).fit()
rOverall.summary()

rOverall2 = smf.ols('crim ~ {}'.format(predictors), data = Boston).fit()
rOverall2.summary()

"""WE can reject the null hypothesis for sure since our F statistic is 33.52.

Now for variables dis, rad, and medv. I would say we can reject the null hypothesis since these all have basically 0 p values. Some variables that need investigated more would be lstat, nox, and zn as there p values are also low but not zero.

(c) How do your results from (a) compare to your results from (b)?
Create a plot displaying the univariate regression coefcients
from (a) on the x-axis, and the multiple regression coefcients
from (b) on the y-axis. That is, each predictor is displayed as a
single point in the plot. Its coefcient in a simple linear regression model is shown on the x-axis, and its coefcient estimate
in the multiple linear regression model is shown on the y-axis.
"""

multi = [rOverall2.params[col] for col in cols]

single = []
for col in cols:
  r= sm.OLS(y, Boston[col]).fit()
  single.append(r.params[0])

plt.figure(figsize = (12,8))
sns.scatterplot(x = single, y = multi)
plt.xlabel('single')
plt.ylabel('multi')
plt.title('single vs multi regression')

"""It seems the coef stayed rather the same for most of the variables except for 2-3 variables changed drastically.

(d) Is there evidence of non-linear association between any of the
predictors and the response? To answer this question, for each
predictor X, fit a model of the form
Y = β0 + β1X + β2X^2 + β3X^3 + e".
"""

for col in cols:
  r = smf.ols('crim ~ {0} + np.power({0}, 2) + np.power({0} , 3)'.format(col), data = Boston).fit()
  print(r.summary())
  print()
  print("-------------------------------------------------------------------")
  print()

models = [smf.ols('crim ~ {0} + np.power({0}, 2) + np.power({0} , 3)'.format(col), data = Boston).fit() for col in cols]

ans = pd.concat([model.pvalues[model.pvalues <0.05] for model in models])
ans

"""There is several significant cells with a p value less than 0.05!"""

